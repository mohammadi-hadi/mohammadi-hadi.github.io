<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<title>Hadi Mohammadi - Publications</title>
<meta http-equiv="Content-Type" content="text/html;charset=UTF-8" />
<link rel="stylesheet" type="text/css" media="screen" href="assets/css/main.css" />
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js" type="text/javascript"></script>
<script src="js/functions.js" type="text/javascript"></script>
<link href="Logo.png" rel="icon">
<link href="Logo.png" rel="apple-touch-icon">
</head>
<body>
<div id="header">
  
</div>


<!--    PUBLICATIONS    --->
<div class="section" id="pubs">
  <h2>Publications & Papers</h2><br><br>
  
  <!-- Forthcoming Publications -->
  <h3 style="color: #0066cc; margin-bottom: 15px;">üìÖ Forthcoming (2025)</h3>
  <div class="item">
    <div class="description" align="justify">
      <p><ul>
        <li><b>Hadi Mohammadi</b>, Tina Shahedi, Pablo Mosteiro Romero, Massimo Poesio, Ayoub Bagheri, Anastasia Giachanou, <i>"Assessing the Reliability of Annotations in the Context of LLMs Predictions and Explanations."</i> <strong>Workshop on Gender Bias in Natural Language Processing (GeBNLP), ACL 2025 conference</strong>, <a href="https://arxiv.org/abs/2507.13138" target="_blank">[arxiv.org]</a></li>
        <li><b>Hadi Mohammadi</b>, Mijntje Meijer, Evi Papadopoulou, Ayoub Bagheri, <i>"Do Large Language Models Understand Morality Across Cultures?"</i> <strong>2nd Workshop on Language Understanding in the Human-Machine Era, ECAI 2025 conference</strong></li>
      </ul></p>
    </div>
  </div>
  
  <!-- Under Review -->
  <h3 style="color: #28a745; margin-bottom: 15px;">üìù Under Review</h3>
  <div class="item">
    <div class="description" align="justify">
      <p><ul>
        <li><b>Hadi Mohammadi</b>, Evi Papadopoulou, Mijntje Meijer, Ayoub Bagheri, <i>"Exploring Cultural Variations in Moral Judgments with Large Language Models."</i> <strong>Under review at Applied Artificial Intelligence journal</strong>, <a href="https://arxiv.org/abs/2506.12433" target="_blank">[arxiv.org]</a></li>
        <li><b>Hadi Mohammadi</b>, Anastasia Giachanou, Daniel Oberski, Ayoub Bagheri, <i>"Explainability-Based Token Replacement on LLM-Generated Text."</i> <strong>Under review at Journal of Artificial Intelligence Research</strong>, <a href="https://arxiv.org/abs/2506.04050" target="_blank">[arxiv.org]</a></li>
        <li><b>Hadi Mohammadi</b>, Ayoub Bagheri, Anastasia Giachanou, Daniel L Oberski, <i>"Explainability in Practice: A Survey of Explainable NLP Across Various Domains."</i> <strong>Under review at Journal of Information Science</strong>, <a href="https://arxiv.org/abs/2502.00837" target="_blank">[arxiv.org]</a></li>
      </ul></p>
    </div>
  </div>
  
  <!-- Published Papers -->
  <h3 style="color: #dc3545; margin-bottom: 15px;">üìñ Published</h3>
  <div class="item">
    <div class="description" align="justify">
      <p><ul>
        <li><b>Hadi Mohammadi</b>, Anastasia Giachanou, Ayoub Bagheri, <i>"A Transparent Pipeline for Identifying Sexism in Social Media: Combining Explainability with Model Prediction."</i> <strong>Applied Sciences Journal</strong> 14.19 (2024): 8620, <a href="https://www.mdpi.com/2076-3417/14/19/8620" target="_blank">[mdpi.com]</a></li>
        <li>Pieter Fivez, Walter Daelemans, Tim Van de Cruys, Yury Kashnitsky, Savvas Chamezopoulos, <b>Hadi Mohammadi</b>, Anastasia Giachanou, Ayoub Bagheri, Wessel Poelman, Juraj Vladika, Esther Ploeger, <i>"The CLIN33 Shared Task on the Detection of Text Generated by Large Language Models."</i> <strong>Computational Linguistics in the Netherlands Journal</strong> 13 (2024): 233-259, <a href="https://clinjournal.org/clinj/article/view/182" target="_blank">[clinjournal.org]</a></li>
        <li><b>Hadi Mohammadi</b>, Anastasia Giachanou, Ayoub Bagheri, <i>"Towards Robust Online Sexism Detection: A Multi-Model Approach with BERT, XLM-RoBERTa, and DistilBERT for EXIST 2023 Tasks."</i> <strong>CLEF 2023 Conference and Labs of the Evaluation Forum</strong>, Thessaloniki, Greece, <a href="https://ceur-ws.org/Vol-3497/paper-085.pdf" target="_blank">[ceur-ws.org]</a></li>
        <li>Mahyar Mirabnejad, <b>Hadi Mohammadi</b>, Mehrdad Mirzabaghi, Amir Aghsami, Fariborz Jolai, Maziar Yazdani, <i>"Home Health Care Problem with Synchronization Visits and Considering Samples Transferring Time: A Case Study in Tehran, Iran."</i> <strong>International Journal of Environmental Research and Public Health</strong> 19.22 (2022): 15036, <a href="https://www.mdpi.com/1660-4601/19/22/15036" target="_blank">[mdpi.com]</a></li>
        <li><b>Hadi Mohammadi</b>, <i>"Statistical Reinforcement Learning with Application in Dynamic Pricing Problem: Learning from Customer Interaction based on Bayesian Approach."</i> <strong>Arshadan Publication</strong>, Tehran (2022)</li>
      </ul></p>
    </div>
  </div>
  
  <!-- Preprints & Working Papers -->
  <h3 style="color: #6610f2; margin-bottom: 15px;">üî¨ Preprints & Working Papers</h3>
  <div class="item">
    <div class="description" align="justify">
      <p><ul>
        <li>Mijntje Meijer, <b>Hadi Mohammadi</b>, Ayoub Bagheri, <i>"LLMs as Mirrors of Societal Moral Standards: Reflection of Cultural Divergence and Agreement Across Ethical Topics."</i> arXiv preprint arXiv:2412.00962 (2024), <a href="https://arxiv.org/abs/2412.00962" target="_blank">[arxiv.org]</a></li>
        <li>Evi Papadopoulou, <b>Hadi Mohammadi</b>, Ayoub Bagheri, <i>"Large Language Models as Mirrors of Societal Moral Standards."</i> arXiv preprint arXiv:2412.00956 (2024), <a href="https://arxiv.org/abs/2412.00956" target="_blank">[arxiv.org]</a></li>
      </ul></p>
    </div>
  </div>
</div>

<!--    Articles    --->
<div class="section">
  <h2>Featured Research</h2><br><br>

  <div class="item">
    <div class="description" align="justify">
      <p><ul>
      <br> <li> <b>TITLE:</b><i>Explainable Sexism Detection in Social Media Considering Human Rationalizations</i>
      <br><b>ABSTRACT:</b> This work presents a transparent pipeline for identifying sexism in social media by combining explainability with model prediction. We develop methods that provide both accurate detection and human-understandable explanations, focusing on the alignment between AI predictions and human reasoning patterns. Our research contributes to making AI systems more trustworthy in sensitive applications like content moderation.
      <br><b>HIGHLIGHTS:</b> <a href="assets/docs/Publications/sexism-detection.png" target="_blank">[View Research]</a> 
      </li>
    </div>
  </div>

  <div class="item">
    <div class="description" align="justify">
      <p><ul>
      <br> <li> <b>TITLE:</b><i>Cultural Variations in Moral Judgments with Large Language Models</i>
      <br><b>ABSTRACT:</b> We explore how Large Language Models understand and represent cultural variations in moral judgments and social norms across different societies. This research aims to make AI systems more culturally aware and fair, investigating whether current LLMs can capture the nuanced moral reasoning patterns that vary across cultures and contexts.
      <br><b>HIGHLIGHTS:</b> <a href="assets/docs/Publications/cultural-moral-judgments.png" target="_blank">[View Research]</a> 
      </li>
    </div>
  </div>

  <div class="item">
    <div class="description" align="justify">
      <p><ul>
      <br> <li> <b>TITLE:</b><i>Assessing Reliability of Annotations in the Context of LLM Predictions and Explanations</i>
      <br><b>ABSTRACT:</b> This study focuses on evaluating the reliability of AI-generated content and annotations, particularly in sensitive domains like hate speech detection. We develop methods to assess when we can trust LLM predictions and explanations, contributing to the broader goal of creating more reliable and safe AI systems for real-world applications.
      <br><b>HIGHLIGHTS:</b> <a href="assets/docs/Publications/annotation-reliability.png" target="_blank">[View Research]</a> 
      </li>
    </div>
  </div>

</div>

</body>
</html>