<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Transparent Pipeline for Identifying Sexism in Social Media</title>
    <meta name="description" content="An explainable AI approach for detecting sexism in social media with transparent decision-making">
    <meta property="og:title" content="A Transparent Pipeline for Identifying Sexism in Social Media">
    <meta property="og:description" content="Combining Explainability with Model Prediction for sexism detection">
    <meta property="og:url" content="https://www.mdpi.com/2076-3417/14/19/8620">
    <meta name="keywords" content="Sexism Detection, Explainable AI, Social Media, NLP, LIME, SHAP, Hate Speech">
    <link rel="icon" type="image/x-icon" href="../../Logo.png">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:opsz,wght@8..60,400;8..60,600;8..60,700&family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <style>
        :root {
            --text-dark: #1a1a1a;
            --text-primary: #374151;
            --text-secondary: #6b7280;
            --text-light: #9ca3af;
            --accent: #0f766e;
            --accent-light: #14b8a6;
            --accent-bg: #f0fdfa;
            --bg-white: #ffffff;
            --bg-light: #f9fafb;
            --border: #e5e7eb;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            line-height: 1.7;
            color: var(--text-primary);
            background: var(--bg-white);
        }

        /* Header */
        .site-header {
            padding: 1rem 2rem;
            border-bottom: 1px solid var(--border);
            position: sticky;
            top: 0;
            background: var(--bg-white);
            z-index: 100;
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .back-link {
            color: var(--accent);
            text-decoration: none;
            font-weight: 500;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.95rem;
        }

        .back-link:hover {
            color: var(--accent-light);
        }

        .header-nav {
            display: flex;
            gap: 1.5rem;
        }

        .header-nav a {
            color: var(--text-secondary);
            text-decoration: none;
            font-size: 0.9rem;
        }

        .header-nav a:hover {
            color: var(--accent);
        }

        /* Hero */
        .hero {
            padding: 4rem 2rem 3rem;
            text-align: center;
            max-width: 900px;
            margin: 0 auto;
        }

        .venue-tag {
            display: inline-block;
            background: var(--accent-bg);
            color: var(--accent);
            padding: 0.4rem 1rem;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            margin-bottom: 1.5rem;
        }

        .hero h1 {
            font-family: 'Source Serif 4', Georgia, serif;
            font-size: 2.5rem;
            font-weight: 700;
            color: var(--text-dark);
            line-height: 1.3;
            margin-bottom: 1.5rem;
        }

        .authors {
            font-size: 1.1rem;
            color: var(--text-primary);
            margin-bottom: 0.5rem;
        }

        .authors a {
            color: var(--text-dark);
            text-decoration: none;
        }

        .authors a:hover {
            color: var(--accent);
        }

        .affiliations {
            color: var(--text-secondary);
            font-size: 0.95rem;
            margin-bottom: 2rem;
        }

        .paper-links {
            display: flex;
            gap: 0.75rem;
            justify-content: center;
            flex-wrap: wrap;
        }

        .paper-link {
            display: inline-flex;
            align-items: center;
            gap: 0.4rem;
            padding: 0.6rem 1.2rem;
            border-radius: 6px;
            text-decoration: none;
            font-size: 0.9rem;
            font-weight: 500;
            transition: all 0.2s;
        }

        .paper-link.primary {
            background: var(--accent);
            color: white;
        }

        .paper-link.primary:hover {
            background: #0d6d66;
        }

        .paper-link.secondary {
            background: var(--bg-light);
            color: var(--text-primary);
            border: 1px solid var(--border);
        }

        .paper-link.secondary:hover {
            border-color: var(--accent);
            color: var(--accent);
        }

        /* Main Content */
        .main-content {
            max-width: 800px;
            margin: 0 auto;
            padding: 0 2rem 4rem;
        }

        .section {
            margin-bottom: 3rem;
        }

        .section-title {
            font-family: 'Source Serif 4', Georgia, serif;
            font-size: 1.5rem;
            font-weight: 600;
            color: var(--text-dark);
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid var(--accent);
            display: inline-block;
        }

        .section p {
            margin-bottom: 1rem;
            text-align: justify;
        }

        .section p:last-child {
            margin-bottom: 0;
        }

        /* Highlights Box */
        .highlights {
            background: var(--bg-light);
            border-left: 4px solid var(--accent);
            padding: 1.5rem;
            margin: 2rem 0;
        }

        .highlights h3 {
            font-size: 1rem;
            font-weight: 600;
            color: var(--text-dark);
            margin-bottom: 1rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .highlights ul {
            list-style: none;
            padding: 0;
        }

        .highlights li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            color: var(--text-primary);
        }

        .highlights li::before {
            content: '';
            position: absolute;
            left: 0;
            top: 1rem;
            width: 8px;
            height: 8px;
            background: var(--accent);
            border-radius: 50%;
        }

        /* Methodology */
        .methodology-grid {
            display: grid;
            gap: 1.5rem;
            margin-top: 1.5rem;
        }

        .method-item {
            display: grid;
            grid-template-columns: auto 1fr;
            gap: 1rem;
            align-items: start;
        }

        .method-number {
            width: 32px;
            height: 32px;
            background: var(--accent);
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 600;
            font-size: 0.9rem;
            flex-shrink: 0;
        }

        .method-content h4 {
            font-size: 1rem;
            font-weight: 600;
            color: var(--text-dark);
            margin-bottom: 0.3rem;
        }

        .method-content p {
            font-size: 0.95rem;
            color: var(--text-secondary);
            margin: 0;
            text-align: left;
        }

        /* Results Table */
        .results-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.95rem;
        }

        .results-table th {
            background: var(--bg-light);
            padding: 0.75rem 1rem;
            text-align: left;
            font-weight: 600;
            color: var(--text-dark);
            border-bottom: 2px solid var(--border);
        }

        .results-table td {
            padding: 0.75rem 1rem;
            border-bottom: 1px solid var(--border);
            color: var(--text-primary);
        }

        .results-table tr:hover {
            background: var(--bg-light);
        }

        .metric-value {
            font-weight: 600;
            color: var(--accent);
        }

        /* XAI Methods */
        .xai-methods {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
            gap: 1.5rem;
            margin-top: 1.5rem;
        }

        .xai-card {
            padding: 1.5rem;
            background: var(--bg-light);
            border-radius: 8px;
        }

        .xai-card h4 {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--accent);
            margin-bottom: 0.75rem;
        }

        .xai-card p {
            font-size: 0.9rem;
            color: var(--text-secondary);
            margin: 0;
            text-align: left;
        }

        /* Key Findings */
        .findings-list {
            margin-top: 1rem;
        }

        .finding-item {
            padding: 1rem 0;
            border-bottom: 1px solid var(--border);
        }

        .finding-item:last-child {
            border-bottom: none;
        }

        .finding-item h4 {
            font-size: 1rem;
            font-weight: 600;
            color: var(--text-dark);
            margin-bottom: 0.5rem;
        }

        .finding-item p {
            font-size: 0.95rem;
            color: var(--text-secondary);
            margin: 0;
            text-align: left;
        }

        /* Citation */
        .citation-block {
            background: var(--bg-light);
            border-radius: 8px;
            padding: 1.5rem;
            position: relative;
        }

        .citation-block pre {
            font-family: 'Monaco', 'Menlo', 'Courier New', monospace;
            font-size: 0.85rem;
            line-height: 1.6;
            overflow-x: auto;
            margin: 0;
            color: var(--text-primary);
        }

        .copy-btn {
            position: absolute;
            top: 1rem;
            right: 1rem;
            padding: 0.4rem 0.8rem;
            background: var(--accent);
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 0.8rem;
            font-weight: 500;
        }

        .copy-btn:hover {
            background: #0d6d66;
        }

        .copy-btn.copied {
            background: #059669;
        }

        /* Footer */
        .site-footer {
            border-top: 1px solid var(--border);
            padding: 2rem;
            text-align: center;
            color: var(--text-secondary);
            font-size: 0.9rem;
        }

        .footer-links {
            display: flex;
            justify-content: center;
            gap: 1.5rem;
            margin-bottom: 1rem;
        }

        .footer-links a {
            color: var(--text-secondary);
            font-size: 1.2rem;
        }

        .footer-links a:hover {
            color: var(--accent);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .hero h1 {
                font-size: 1.8rem;
            }

            .header-nav {
                display: none;
            }

            .main-content {
                padding: 0 1.5rem 3rem;
            }

            .xai-methods {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <header class="site-header">
        <div class="header-content">
            <a href="../../index.html" class="back-link">
                <i class="fas fa-arrow-left"></i> Hadi Mohammadi
            </a>
            <nav class="header-nav">
                <a href="#abstract">Abstract</a>
                <a href="#methodology">Method</a>
                <a href="#results">Results</a>
                <a href="#citation">Cite</a>
            </nav>
        </div>
    </header>

    <section class="hero">
        <span class="venue-tag">
            <i class="fas fa-journal-whills"></i> Applied Sciences 2024
        </span>
        <h1>A Transparent Pipeline for Identifying Sexism in Social Media: Combining Explainability with Model Prediction</h1>
        <p class="authors">
            <a href="../../index.html"><strong>Hadi Mohammadi</strong></a>,
            Anastasia Giachanou,
            Ayoub Bagheri
        </p>
        <p class="affiliations">Utrecht University, The Netherlands</p>
        <div class="paper-links">
            <a href="https://www.mdpi.com/2076-3417/14/19/8620" class="paper-link primary" target="_blank">
                <i class="fas fa-file-pdf"></i> Paper
            </a>
            <a href="https://github.com/mohammadi-hadi/Explainable-Sexim-Detection" class="paper-link secondary" target="_blank">
                <i class="fab fa-github"></i> Code
            </a>
            <a href="https://www.mdpi.com/2076-3417/14/19/8620" class="paper-link secondary" target="_blank">
                <i class="fas fa-external-link-alt"></i> Journal
            </a>
        </div>
    </section>

    <main class="main-content">
        <!-- Abstract -->
        <section id="abstract" class="section">
            <h2 class="section-title">Abstract</h2>
            <p>
                Sexism, a form of discrimination based on gender, is increasingly prevalent on social media platforms, where it often manifests as hate speech targeted at individuals or groups based on their gender. While machine learning models can detect such content, their "black box" nature obscures their decision-making processes, making it difficult for users to understand why certain posts are flagged as sexist.
            </p>
            <p>
                This paper addresses the critical need for transparency in automated sexism detection by proposing an explainable pipeline that combines accurate classification with interpretable explanations. We demonstrate that incorporating explainability techniques like LIME and SHAP not only maintains high detection accuracy but also provides valuable insights into model behavior, revealing which words and phrases most strongly indicate sexist content.
            </p>
            <p>
                Our comprehensive evaluation on the EXIST 2021 dataset shows that our transparent approach achieves an F1-score of 0.82 while providing clear, understandable explanations for each prediction. This dual focus on accuracy and interpretability makes our system particularly suitable for real-world deployment, where understanding the reasoning behind content moderation decisions is crucial for both platform operators and users.
            </p>
        </section>

        <!-- Key Contributions -->
        <div class="highlights">
            <h3>Key Contributions</h3>
            <ul>
                <li><strong>Transparent Detection Pipeline:</strong> A complete framework combining RoBERTa classification with multiple XAI techniques for sexism detection</li>
                <li><strong>Multi-Method Explainability:</strong> Systematic comparison of LIME, SHAP, and attention-based explanations for text classification</li>
                <li><strong>Real-World Applicability:</strong> Practical guidelines for deploying explainable hate speech detection in content moderation systems</li>
                <li><strong>Linguistic Insights:</strong> Analysis of linguistic patterns and keywords most indicative of sexist content across different contexts</li>
            </ul>
        </div>

        <!-- Methodology -->
        <section id="methodology" class="section">
            <h2 class="section-title">Methodology</h2>
            <p>
                Our approach integrates state-of-the-art transformer-based classification with post-hoc explainability techniques to create a transparent detection system. The pipeline processes social media text through multiple stages to ensure both accurate detection and interpretable outputs.
            </p>

            <div class="methodology-grid">
                <div class="method-item">
                    <div class="method-number">1</div>
                    <div class="method-content">
                        <h4>Data Preprocessing</h4>
                        <p>Text normalization, handling of social media artifacts (mentions, hashtags, URLs), and tokenization optimized for transformer models.</p>
                    </div>
                </div>
                <div class="method-item">
                    <div class="method-number">2</div>
                    <div class="method-content">
                        <h4>RoBERTa Classification</h4>
                        <p>Fine-tuned RoBERTa-base model for binary sexism classification, trained on the EXIST 2021 dataset with careful hyperparameter optimization.</p>
                    </div>
                </div>
                <div class="method-item">
                    <div class="method-number">3</div>
                    <div class="method-content">
                        <h4>Explanation Generation</h4>
                        <p>Application of LIME, SHAP, and attention extraction to identify the most influential words and phrases for each prediction.</p>
                    </div>
                </div>
                <div class="method-item">
                    <div class="method-number">4</div>
                    <div class="method-content">
                        <h4>Explanation Evaluation</h4>
                        <p>Quantitative assessment of explanation quality using faithfulness metrics and human evaluation studies.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- XAI Methods -->
        <section class="section">
            <h2 class="section-title">Explainability Techniques</h2>
            <p>
                We employ three complementary explainability approaches to provide comprehensive insights into model decisions:
            </p>

            <div class="xai-methods">
                <div class="xai-card">
                    <h4>LIME</h4>
                    <p>Local Interpretable Model-agnostic Explanations. Perturbs input text to identify which words most influence the prediction through local linear approximation.</p>
                </div>
                <div class="xai-card">
                    <h4>SHAP</h4>
                    <p>SHapley Additive exPlanations. Computes fair attribution scores based on game-theoretic principles, providing consistent feature importance values.</p>
                </div>
                <div class="xai-card">
                    <h4>Attention Weights</h4>
                    <p>Extracts attention patterns from transformer layers to visualize which tokens the model focuses on during classification.</p>
                </div>
            </div>
        </section>

        <!-- Results -->
        <section id="results" class="section">
            <h2 class="section-title">Results</h2>
            <p>
                Our transparent pipeline achieves competitive performance on the EXIST 2021 benchmark while providing interpretable explanations for each prediction.
            </p>

            <table class="results-table">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Score</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>F1-Score</td>
                        <td class="metric-value">0.82</td>
                        <td>Harmonic mean of precision and recall</td>
                    </tr>
                    <tr>
                        <td>Precision</td>
                        <td class="metric-value">0.84</td>
                        <td>Accuracy of positive predictions</td>
                    </tr>
                    <tr>
                        <td>Recall</td>
                        <td class="metric-value">0.80</td>
                        <td>Coverage of actual sexist content</td>
                    </tr>
                    <tr>
                        <td>Accuracy</td>
                        <td class="metric-value">0.83</td>
                        <td>Overall classification accuracy</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- Key Findings -->
        <section class="section">
            <h2 class="section-title">Key Findings</h2>

            <div class="findings-list">
                <div class="finding-item">
                    <h4>Linguistic Patterns in Sexist Content</h4>
                    <p>Our explainability analysis reveals that sexist content often relies on specific linguistic patterns including gendered slurs, objectifying language, and stereotypical role assignments. These patterns are consistently identified across all three XAI methods.</p>
                </div>
                <div class="finding-item">
                    <h4>Complementary Explanation Methods</h4>
                    <p>LIME and SHAP provide complementary perspectives: LIME excels at identifying local decision boundaries while SHAP offers more consistent global feature importance. Combining both yields the most comprehensive understanding.</p>
                </div>
                <div class="finding-item">
                    <h4>Context-Dependent Detection</h4>
                    <p>The model successfully captures context-dependent sexism where the same words may or may not indicate sexist content depending on surrounding context, demonstrating sophisticated linguistic understanding.</p>
                </div>
                <div class="finding-item">
                    <h4>Cross-Lingual Patterns</h4>
                    <p>Analysis of Spanish and English tweets reveals both universal sexist patterns and language-specific expressions, highlighting the need for multilingual approaches in content moderation.</p>
                </div>
            </div>
        </section>

        <!-- Implications -->
        <section class="section">
            <h2 class="section-title">Implications for Practice</h2>
            <p>
                Our findings have important implications for deploying AI-based content moderation systems. The combination of high accuracy and interpretable explanations enables content moderators to review and verify automated decisions efficiently. This transparency is crucial for building user trust and ensuring accountability in platform governance.
            </p>
            <p>
                The explainability pipeline can also support policy development by revealing the linguistic characteristics of sexist content, helping platforms refine their community guidelines and moderation policies based on empirical evidence rather than intuition alone.
            </p>
        </section>

        <!-- Citation -->
        <section id="citation" class="section">
            <h2 class="section-title">Citation</h2>
            <div class="citation-block">
                <button class="copy-btn" onclick="copyBibtex()">Copy</button>
                <pre id="bibtex">@article{mohammadi2024transparent,
  title={A Transparent Pipeline for Identifying Sexism in Social Media:
         Combining Explainability with Model Prediction},
  author={Mohammadi, Hadi and Giachanou, Anastasia and Bagheri, Ayoub},
  journal={Applied Sciences},
  volume={14},
  number={19},
  pages={8620},
  year={2024},
  publisher={MDPI}
}</pre>
            </div>
        </section>
    </main>

    <footer class="site-footer">
        <div class="footer-links">
            <a href="../../index.html" title="Home"><i class="fas fa-home"></i></a>
            <a href="https://github.com/mohammadi-hadi" title="GitHub" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://scholar.google.com/citations?user=YOUR_ID" title="Google Scholar" target="_blank"><i class="fas fa-graduation-cap"></i></a>
        </div>
        <p>&copy; 2024 Hadi Mohammadi</p>
    </footer>

    <script>
        function copyBibtex() {
            const bibtex = document.getElementById('bibtex').innerText;
            const btn = document.querySelector('.copy-btn');

            navigator.clipboard.writeText(bibtex).then(() => {
                btn.textContent = 'Copied!';
                btn.classList.add('copied');
                setTimeout(() => {
                    btn.textContent = 'Copy';
                    btn.classList.remove('copied');
                }, 2000);
            });
        }
    </script>
</body>
</html>
