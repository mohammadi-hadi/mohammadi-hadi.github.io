<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assessing the Reliability of LLMs Annotations</title>
    <meta name="description" content="Investigating the reliability of LLM-generated annotations for bias detection and explainability">
    <meta property="og:title" content="Assessing the Reliability of LLMs Annotations">
    <meta property="og:description" content="In the Context of Demographic Bias and Model Explanation">
    <meta property="og:url" content="https://arxiv.org/abs/2507.13138">
    <meta name="keywords" content="LLM, Demographic Bias, Model Explanation, NLP, Sexism Detection, Annotation Reliability, XAI, Fairness">
    <link rel="icon" type="image/x-icon" href="../../Logo.png">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:opsz,wght@8..60,400;8..60,600;8..60,700&family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <style>
        :root {
            --text-dark: #1a1a1a;
            --text-primary: #374151;
            --text-secondary: #6b7280;
            --accent: #0f766e;
            --accent-light: #14b8a6;
            --accent-bg: #f0fdfa;
            --bg-white: #ffffff;
            --bg-light: #f9fafb;
            --border: #e5e7eb;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            line-height: 1.7;
            color: var(--text-primary);
            background: var(--bg-white);
        }

        .site-header {
            padding: 1rem 2rem;
            border-bottom: 1px solid var(--border);
            position: sticky;
            top: 0;
            background: var(--bg-white);
            z-index: 100;
        }

        .header-content {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .back-link {
            color: var(--accent);
            text-decoration: none;
            font-weight: 500;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.95rem;
        }
        .back-link:hover { color: var(--accent-light); }

        .header-nav { display: flex; gap: 1.5rem; }
        .header-nav a { color: var(--text-secondary); text-decoration: none; font-size: 0.9rem; }
        .header-nav a:hover { color: var(--accent); }

        .hero { padding: 4rem 2rem 3rem; text-align: center; max-width: 900px; margin: 0 auto; }

        .venue-tag {
            display: inline-block;
            background: var(--accent-bg);
            color: var(--accent);
            padding: 0.4rem 1rem;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 500;
            margin-bottom: 1.5rem;
        }

        .hero h1 {
            font-family: 'Source Serif 4', Georgia, serif;
            font-size: 2.5rem;
            font-weight: 700;
            color: var(--text-dark);
            line-height: 1.3;
            margin-bottom: 1.5rem;
        }

        .authors { font-size: 1.1rem; color: var(--text-primary); margin-bottom: 0.5rem; }
        .authors a { color: var(--text-dark); text-decoration: none; }
        .authors a:hover { color: var(--accent); }
        .affiliations { color: var(--text-secondary); font-size: 0.95rem; margin-bottom: 2rem; }

        .paper-links { display: flex; gap: 0.75rem; justify-content: center; flex-wrap: wrap; }
        .paper-link {
            display: inline-flex; align-items: center; gap: 0.4rem;
            padding: 0.6rem 1.2rem; border-radius: 6px;
            text-decoration: none; font-size: 0.9rem; font-weight: 500; transition: all 0.2s;
        }
        .paper-link.primary { background: var(--accent); color: white; }
        .paper-link.primary:hover { background: #0d6d66; }
        .paper-link.secondary { background: var(--bg-light); color: var(--text-primary); border: 1px solid var(--border); }
        .paper-link.secondary:hover { border-color: var(--accent); color: var(--accent); }

        .main-content { max-width: 800px; margin: 0 auto; padding: 0 2rem 4rem; }
        .section { margin-bottom: 3rem; }

        .section-title {
            font-family: 'Source Serif 4', Georgia, serif;
            font-size: 1.5rem; font-weight: 600; color: var(--text-dark);
            margin-bottom: 1rem; padding-bottom: 0.5rem;
            border-bottom: 2px solid var(--accent); display: inline-block;
        }

        .section p { margin-bottom: 1rem; text-align: justify; }
        .section p:last-child { margin-bottom: 0; }

        .highlights {
            background: var(--bg-light); border-left: 4px solid var(--accent);
            padding: 1.5rem; margin: 2rem 0;
        }
        .highlights h3 {
            font-size: 1rem; font-weight: 600; color: var(--text-dark);
            margin-bottom: 1rem; text-transform: uppercase; letter-spacing: 0.5px;
        }
        .highlights ul { list-style: none; padding: 0; }
        .highlights li { padding: 0.5rem 0; padding-left: 1.5rem; position: relative; color: var(--text-primary); }
        .highlights li::before {
            content: ''; position: absolute; left: 0; top: 1rem;
            width: 8px; height: 8px; background: var(--accent); border-radius: 50%;
        }

        .methodology-grid { display: grid; gap: 1.5rem; margin-top: 1.5rem; }
        .method-item { display: grid; grid-template-columns: auto 1fr; gap: 1rem; align-items: start; }
        .method-number {
            width: 32px; height: 32px; background: var(--accent); color: white;
            border-radius: 50%; display: flex; align-items: center; justify-content: center;
            font-weight: 600; font-size: 0.9rem; flex-shrink: 0;
        }
        .method-content h4 { font-size: 1rem; font-weight: 600; color: var(--text-dark); margin-bottom: 0.3rem; }
        .method-content p { font-size: 0.95rem; color: var(--text-secondary); margin: 0; text-align: left; }

        .results-table { width: 100%; border-collapse: collapse; margin: 1.5rem 0; font-size: 0.95rem; }
        .results-table th {
            background: var(--bg-light); padding: 0.75rem 1rem; text-align: left;
            font-weight: 600; color: var(--text-dark); border-bottom: 2px solid var(--border);
        }
        .results-table td { padding: 0.75rem 1rem; border-bottom: 1px solid var(--border); color: var(--text-primary); }
        .results-table tr:hover { background: var(--bg-light); }
        .metric-value { font-weight: 600; color: var(--accent); }

        .model-grid {
            display: grid; grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
            gap: 1.5rem; margin-top: 1.5rem;
        }
        .model-card { padding: 1.5rem; background: var(--bg-light); border-radius: 8px; }
        .model-card h4 { font-size: 1.1rem; font-weight: 600; color: var(--accent); margin-bottom: 0.75rem; }
        .model-card p { font-size: 0.9rem; color: var(--text-secondary); margin: 0; text-align: left; }
        .model-card .score { font-size: 1.5rem; font-weight: 700; color: var(--accent); margin-bottom: 0.5rem; }

        .findings-list { margin-top: 1rem; }
        .finding-item { padding: 1rem 0; border-bottom: 1px solid var(--border); }
        .finding-item:last-child { border-bottom: none; }
        .finding-item h4 { font-size: 1rem; font-weight: 600; color: var(--text-dark); margin-bottom: 0.5rem; }
        .finding-item p { font-size: 0.95rem; color: var(--text-secondary); margin: 0; text-align: left; }

        .recommendation-list { counter-reset: rec-counter; margin-top: 1.5rem; }
        .recommendation-item {
            display: flex; gap: 1rem; padding: 1rem; margin-bottom: 1rem;
            background: var(--bg-light); border-radius: 8px;
        }
        .rec-number {
            width: 28px; height: 28px; background: var(--accent); color: white;
            border-radius: 50%; display: flex; align-items: center; justify-content: center;
            font-weight: 600; font-size: 0.85rem; flex-shrink: 0;
        }
        .recommendation-item p { font-size: 0.95rem; color: var(--text-primary); margin: 0; }

        .citation-block { background: var(--bg-light); border-radius: 8px; padding: 1.5rem; position: relative; }
        .citation-block pre {
            font-family: 'Monaco', 'Menlo', 'Courier New', monospace;
            font-size: 0.85rem; line-height: 1.6; overflow-x: auto; margin: 0; color: var(--text-primary);
        }
        .copy-btn {
            position: absolute; top: 1rem; right: 1rem; padding: 0.4rem 0.8rem;
            background: var(--accent); color: white; border: none; border-radius: 4px;
            cursor: pointer; font-size: 0.8rem; font-weight: 500;
        }
        .copy-btn:hover { background: #0d6d66; }
        .copy-btn.copied { background: #059669; }

        .site-footer {
            border-top: 1px solid var(--border); padding: 2rem;
            text-align: center; color: var(--text-secondary); font-size: 0.9rem;
        }
        .footer-links { display: flex; justify-content: center; gap: 1.5rem; margin-bottom: 1rem; }
        .footer-links a { color: var(--text-secondary); font-size: 1.2rem; }
        .footer-links a:hover { color: var(--accent); }

        @media (max-width: 768px) {
            .hero h1 { font-size: 1.8rem; }
            .header-nav { display: none; }
            .main-content { padding: 0 1.5rem 3rem; }
            .model-grid { grid-template-columns: 1fr; }
        }
    </style>
</head>
<body>
    <header class="site-header">
        <div class="header-content">
            <a href="../../index.html" class="back-link">
                <i class="fas fa-arrow-left"></i> Hadi Mohammadi
            </a>
            <nav class="header-nav">
                <a href="#abstract">Abstract</a>
                <a href="#methodology">Method</a>
                <a href="#results">Results</a>
                <a href="#citation">Cite</a>
            </nav>
        </div>
    </header>

    <section class="hero">
        <span class="venue-tag">
            <i class="fas fa-users"></i> ACL 2025 - GeBNLP Workshop
        </span>
        <h1>Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation</h1>
        <p class="authors">
            <a href="../../index.html"><strong>Hadi Mohammadi</strong></a>,
            Fatemeh Shahedi, Pablo Mosteiro Romero, Massimo Poesio,
            Ayoub Bagheri, Anastasia Giachanou
        </p>
        <p class="affiliations">Utrecht University &amp; Queen Mary University of London</p>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2507.13138" class="paper-link primary" target="_blank">
                <i class="fas fa-file-pdf"></i> Paper
            </a>
            <a href="https://github.com/mohammadi-hadi/LLM-Annotation-Reliability" class="paper-link secondary" target="_blank">
                <i class="fab fa-github"></i> Code
            </a>
            <a href="https://arxiv.org/abs/2507.13138" class="paper-link secondary" target="_blank">
                <i class="fas fa-external-link-alt"></i> arXiv
            </a>
        </div>
    </section>

    <main class="main-content">
        <section id="abstract" class="section">
            <h2 class="section-title">Abstract</h2>
            <p>
                Large Language Models (LLMs) are increasingly used as annotators for NLP tasks, promising cost-effective alternatives to human annotation. However, the reliability of LLM-generated annotations, particularly in sensitive domains like bias detection, remains understudied. This paper presents a comprehensive investigation into the reliability of LLM annotations for sexism detection, examining both classification accuracy and the quality of explanations provided.
            </p>
            <p>
                We evaluate multiple state-of-the-art LLMs (GPT-4, Claude, Llama) on their ability to detect sexist content and provide faithful explanations for their decisions. Our analysis reveals significant variation in annotation reliability across models and demographic groups, with concerning patterns of bias in how different LLMs perceive and explain sexist content.
            </p>
            <p>
                Our findings demonstrate that while LLMs can achieve reasonable classification accuracy, their explanations often lack faithfulness to actual decision factors, and their annotations exhibit systematic biases that vary by target demographic. These results have important implications for the responsible use of LLMs in annotation pipelines, particularly for sensitive content moderation tasks.
            </p>
        </section>

        <div class="highlights">
            <h3>Key Contributions</h3>
            <ul>
                <li><strong>Comprehensive Evaluation:</strong> First systematic study of LLM annotation reliability for bias detection across multiple models</li>
                <li><strong>Explanation Faithfulness:</strong> Novel analysis of whether LLM explanations accurately reflect their decision-making processes</li>
                <li><strong>Demographic Bias Analysis:</strong> Investigation of how LLM annotations vary across different target demographics</li>
                <li><strong>Practical Guidelines:</strong> Recommendations for responsible deployment of LLMs in annotation pipelines</li>
            </ul>
        </div>

        <section id="methodology" class="section">
            <h2 class="section-title">Methodology</h2>
            <p>
                We design a rigorous evaluation framework to assess both the accuracy and reliability of LLM-generated annotations for sexism detection.
            </p>

            <div class="methodology-grid">
                <div class="method-item">
                    <div class="method-number">1</div>
                    <div class="method-content">
                        <h4>Dataset Preparation</h4>
                        <p>Curate a balanced dataset of sexist and non-sexist content with human annotations and demographic metadata.</p>
                    </div>
                </div>
                <div class="method-item">
                    <div class="method-number">2</div>
                    <div class="method-content">
                        <h4>LLM Annotation Collection</h4>
                        <p>Collect annotations and explanations from GPT-4, Claude, and Llama using standardized prompting strategies.</p>
                    </div>
                </div>
                <div class="method-item">
                    <div class="method-number">3</div>
                    <div class="method-content">
                        <h4>Reliability Assessment</h4>
                        <p>Measure inter-annotator agreement, classification metrics, and explanation quality across models.</p>
                    </div>
                </div>
                <div class="method-item">
                    <div class="method-number">4</div>
                    <div class="method-content">
                        <h4>Bias Analysis</h4>
                        <p>Examine systematic differences in annotations across target demographics and content types.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="section">
            <h2 class="section-title">Model Comparison</h2>
            <p>
                We evaluate three leading LLMs on their sexism detection performance:
            </p>

            <div class="model-grid">
                <div class="model-card">
                    <div class="score">0.84 F1</div>
                    <h4>GPT-4</h4>
                    <p>Highest classification accuracy but shows significant variation in explanation quality across demographics.</p>
                </div>
                <div class="model-card">
                    <div class="score">0.81 F1</div>
                    <h4>Claude</h4>
                    <p>Most consistent explanations but slightly lower classification performance. Better calibrated uncertainty.</p>
                </div>
                <div class="model-card">
                    <div class="score">0.76 F1</div>
                    <h4>Llama 3</h4>
                    <p>Lower overall performance but interesting patterns in how it handles ambiguous cases.</p>
                </div>
            </div>
        </section>

        <section id="results" class="section">
            <h2 class="section-title">Results</h2>

            <table class="results-table">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>GPT-4</th>
                        <th>Claude</th>
                        <th>Llama 3</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>F1-Score</td>
                        <td class="metric-value">0.84</td>
                        <td class="metric-value">0.81</td>
                        <td class="metric-value">0.76</td>
                    </tr>
                    <tr>
                        <td>Explanation Faithfulness</td>
                        <td>0.72</td>
                        <td class="metric-value">0.78</td>
                        <td>0.65</td>
                    </tr>
                    <tr>
                        <td>Demographic Consistency</td>
                        <td>0.68</td>
                        <td class="metric-value">0.75</td>
                        <td>0.62</td>
                    </tr>
                    <tr>
                        <td>Agreement with Humans</td>
                        <td class="metric-value">0.79</td>
                        <td>0.76</td>
                        <td>0.71</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section class="section">
            <h2 class="section-title">Key Findings</h2>

            <div class="findings-list">
                <div class="finding-item">
                    <h4>Explanation-Decision Mismatch</h4>
                    <p>LLM explanations frequently cite features that are not actually predictive of their classifications. This "explanation faithfulness gap" is particularly pronounced for edge cases and ambiguous content.</p>
                </div>
                <div class="finding-item">
                    <h4>Demographic Bias in Annotations</h4>
                    <p>All models show systematic differences in how they annotate content targeting different demographic groups. Content targeting women receives higher sexism scores than equivalent content targeting other groups.</p>
                </div>
                <div class="finding-item">
                    <h4>Inconsistent Calibration</h4>
                    <p>LLMs express high confidence even when their predictions are incorrect. Claude shows the best calibration, while GPT-4 tends toward overconfidence.</p>
                </div>
                <div class="finding-item">
                    <h4>Context Sensitivity</h4>
                    <p>LLMs struggle with context-dependent sexism where meaning changes based on speaker identity or conversational context, often defaulting to surface-level pattern matching.</p>
                </div>
            </div>
        </section>

        <section class="section">
            <h2 class="section-title">Recommendations</h2>
            <p>Based on our findings, we provide guidelines for using LLMs in annotation pipelines:</p>

            <div class="recommendation-list">
                <div class="recommendation-item">
                    <span class="rec-number">1</span>
                    <p><strong>Human-in-the-Loop:</strong> Maintain human oversight for sensitive annotation tasks, using LLMs for initial filtering rather than final decisions.</p>
                </div>
                <div class="recommendation-item">
                    <span class="rec-number">2</span>
                    <p><strong>Multi-Model Ensemble:</strong> Use multiple LLMs and aggregate their annotations to reduce individual model biases.</p>
                </div>
                <div class="recommendation-item">
                    <span class="rec-number">3</span>
                    <p><strong>Explanation Verification:</strong> Don't trust LLM explanations at face value; validate them against actual prediction factors.</p>
                </div>
                <div class="recommendation-item">
                    <span class="rec-number">4</span>
                    <p><strong>Demographic Auditing:</strong> Regularly audit LLM annotations for demographic disparities before deployment.</p>
                </div>
            </div>
        </section>

        <section id="citation" class="section">
            <h2 class="section-title">Citation</h2>
            <div class="citation-block">
                <button class="copy-btn" onclick="copyBibtex()">Copy</button>
                <pre id="bibtex">@inproceedings{mohammadi2025assessing,
  title={Assessing the Reliability of LLMs Annotations in the
         Context of Demographic Bias and Model Explanation},
  author={Mohammadi, Hadi and Shahedi, Fatemeh and
          Mosteiro Romero, Pablo and Poesio, Massimo and
          Bagheri, Ayoub and Giachanou, Anastasia},
  booktitle={Proceedings of the ACL 2025 Workshop on
             Gender Bias in Natural Language Processing},
  year={2025}
}</pre>
            </div>
        </section>
    </main>

    <footer class="site-footer">
        <div class="footer-links">
            <a href="../../index.html" title="Home"><i class="fas fa-home"></i></a>
            <a href="https://github.com/mohammadi-hadi" title="GitHub" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://scholar.google.com/citations?user=YOUR_ID" title="Google Scholar" target="_blank"><i class="fas fa-graduation-cap"></i></a>
        </div>
        <p>&copy; 2025 Hadi Mohammadi</p>
    </footer>

    <script>
        function copyBibtex() {
            const bibtex = document.getElementById('bibtex').innerText;
            const btn = document.querySelector('.copy-btn');
            navigator.clipboard.writeText(bibtex).then(() => {
                btn.textContent = 'Copied!';
                btn.classList.add('copied');
                setTimeout(() => { btn.textContent = 'Copy'; btn.classList.remove('copied'); }, 2000);
            });
        }
    </script>
</body>
</html>
