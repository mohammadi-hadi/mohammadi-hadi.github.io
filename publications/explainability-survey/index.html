<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Explainability in Practice: A Survey of Explainable NLP Across Various Domains">
  <meta property="og:title" content="Explainability in Practice: A Survey of Explainable NLP"/>
  <meta property="og:description" content="A comprehensive survey examining the current state of explainable NLP across various domains"/>
  <meta property="og:url" content="https://arxiv.org/abs/2502.00837"/>
  <meta name="twitter:title" content="Explainability in Practice: A Survey of Explainable NLP">
  <meta name="twitter:description" content="A comprehensive survey examining the current state of explainable NLP across various domains">
  <meta name="keywords" content="Explainable AI, NLP, Interpretability, Survey, Machine Learning, XAI">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Explainability in Practice: A Survey of Explainable NLP</title>
  <link rel="icon" type="image/x-icon" href="../../Logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
  <style>
    .hero-body {
      padding: 3rem 1.5rem;
    }
    .publication-authors {
      font-size: 1.2rem;
      color: #666;
      margin-bottom: 1rem;
    }
    .publication-venue {
      font-size: 1.1rem;
      color: #888;
      font-style: italic;
    }
    .content-section {
      padding: 3rem 0;
    }
    .abstract-box {
      background-color: #f8f9fa;
      padding: 2rem;
      border-radius: 8px;
      margin: 2rem 0;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Explainability in Practice: A Survey of Explainable NLP Across Various Domains</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <strong>Hadi Mohammadi</strong><sup>1</sup>,
            </span>
            <span class="author-block">
              Ayoub Bagheri<sup>1</sup>,
            </span>
            <span class="author-block">
              Anastasia Giachanou<sup>1</sup>,
            </span>
            <span class="author-block">
              Daniel L. Oberski<sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Utrecht University, The Netherlands</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.00837"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.00837"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="../../index.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-home"></i>
                  </span>
                  <span>Back to Main</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section content-section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="abstract-box">
          <div class="content has-text-justified">
            <p>
              The rapid advancement of Natural Language Processing (NLP) models has led to their widespread adoption across various domains. However, the black-box nature of these models raises concerns about their transparency and trustworthiness, particularly in high-stakes applications. This survey provides a comprehensive examination of the current state of explainable NLP across various domains, analyzing over 200 recent papers to understand how explainability is being implemented and evaluated in practice.
            </p>
            <p>
              We categorize existing approaches based on their explainability methods, target audiences, and application domains. Our analysis reveals significant gaps between theoretical explainability frameworks and their practical implementations, particularly in domain-specific contexts. We identify key challenges including the lack of standardized evaluation metrics, the trade-off between model performance and interpretability, and the difficulty in generating explanations that are both faithful to the model and understandable to end-users.
            </p>
            <p>
              This survey contributes to the field by: (1) providing a structured taxonomy of explainable NLP methods, (2) analyzing domain-specific requirements and implementations, (3) identifying current limitations and future research directions, and (4) proposing guidelines for developing more effective explainable NLP systems. Our findings emphasize the need for collaborative efforts between NLP researchers and domain experts to create truly useful and trustworthy AI systems.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section content-section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Key Contributions</h2>
        <div class="content has-text-justified">
          <ul>
            <li><strong>Comprehensive Taxonomy:</strong> We develop a structured taxonomy categorizing explainable NLP methods based on their approach (post-hoc vs. intrinsic), granularity (token, sentence, document level), and target audience (researchers, practitioners, end-users).</li>
            <li><strong>Domain Analysis:</strong> We analyze how explainability requirements and implementations vary across different domains including healthcare, finance, legal, and social media analysis.</li>
            <li><strong>Evaluation Framework:</strong> We propose a unified framework for evaluating explainable NLP systems, considering both technical metrics (faithfulness, consistency) and human-centered metrics (understandability, usefulness).</li>
            <li><strong>Practical Guidelines:</strong> Based on our analysis, we provide practical guidelines for researchers and practitioners developing explainable NLP systems.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section content-section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Survey Methodology</h2>
        <div class="content has-text-justified">
          <p>
            Our survey follows a systematic literature review methodology, analyzing papers published between 2020 and 2024 in top-tier NLP and AI venues. We focus on papers that explicitly address explainability in NLP applications, excluding purely theoretical works without practical implementations.
          </p>
          <p>
            The selection criteria included: (1) papers presenting explainable NLP methods or systems, (2) empirical evaluations of explainability approaches, (3) domain-specific applications of explainable NLP, and (4) user studies on NLP explanations.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section content-section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Key Findings</h2>
        <div class="content has-text-justified">
          <ol>
            <li><strong>Gap Between Theory and Practice:</strong> While numerous explainability methods exist, their practical implementation often falls short of theoretical promises, particularly in real-world applications.</li>
            <li><strong>Domain-Specific Requirements:</strong> Different domains have vastly different explainability needs - what works for sentiment analysis may not be suitable for medical diagnosis.</li>
            <li><strong>Evaluation Challenges:</strong> There is no consensus on how to evaluate explainability, leading to inconsistent and incomparable results across studies.</li>
            <li><strong>User-Centric Design:</strong> Most explainability methods are designed by and for ML researchers, often failing to meet the needs of actual end-users.</li>
            <li><strong>Trade-offs:</strong> There are inherent trade-offs between model performance, explainability, and computational efficiency that need careful consideration.</li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section content-section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Citation</h2>
        <div class="content">
          <pre><code>@article{mohammadi2025explainability,
  title={Explainability in Practice: A Survey of Explainable NLP Across Various Domains},
  author={Mohammadi, Hadi and Bagheri, Ayoub and Giachanou, Anastasia and Oberski, Daniel L.},
  journal={arXiv preprint arXiv:2502.00837},
  year={2025}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="../../index.html">
        <i class="fas fa-home"></i>
      </a>
      <a class="icon-link" href="https://github.com/mohammadi-hadi" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>